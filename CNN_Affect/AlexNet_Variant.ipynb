{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture 1: AlexNet Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, GaussianDropout, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import Sequential, load_model\n",
    "# from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import tensorflow as tf\n",
    "# import autokeras as ak\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "- A series of incremental smaller convolution kernels feeding through the Convolutional (Conventional 2D) layer:\n",
    "    - 9 x 9 x 16\n",
    "    - 7 x 7 x 32\n",
    "    - 5 x 5 x 64\n",
    "    - 3 x 3 x 128\n",
    "- Each Conv2D layer is followed by a batch normalization layer and a ReLU activation layer\n",
    "- A 2 x 2 max-pooling layer between two Convolutional layers\n",
    "- GausianDropout at\n",
    "    - 0.2 rate after each pooling layer\n",
    "    - 0.5 rate after each dense layer\n",
    "- flatten\n",
    "- 2 fully connected Dense layers before the output layer\n",
    "\n",
    "- Output layer:\n",
    "    - return the valence/arousal pair, also **regression = True**: input_shape = 2, linear activation\n",
    "        - compile: mean_squared_error, adam optimizer\n",
    "        - return 2 floats (valence, arousal)\n",
    "    - return the Expression classification, also **regression = False**: input_shape = 8, softmax activation\n",
    "        - compile: categorical_crossentropy, adam optimizer, metrics = ['accuracy']\n",
    "        - return 1 label (0 to 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments:\n",
    "#   image_size: assuming all the input photo has size m*m. The default of this dataset is 128\n",
    "#   R_or_C: regression (0) or classification (1). We are more interested in regression so the default is set to 0\n",
    "def alexnet_var_model(image_size = 128,\n",
    "                      regression = True, \n",
    "                      conv_shapes = [[16, (9, 9)], [32, (7, 7)], [64, (5, 5)], [128, (3, 3)], [128, (3, 3)]], \n",
    "                      dropout=[0.2, 0.5]) :\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolution blocks\n",
    "    for i in range (len(conv_shapes)) :\n",
    "        if (i==0) : \n",
    "            # the package requires input_shape when Conv2D is the first layer of the model\n",
    "            model.add(Conv2D(conv_shapes[i][0], conv_shapes[i][1], input_shape=(image_size, image_size, 3), padding='same', use_bias=False))\n",
    "        else:\n",
    "            model.add(Conv2D(conv_shapes[i][0], conv_shapes[i][1], padding='same', use_bias=False))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "        model.add(GaussianDropout(dropout[0]))\n",
    "\n",
    "\n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Adding 2 Dense layers\n",
    "    for i in range (2) :\n",
    "        model.add(Dense(1024))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(GaussianDropout(dropout[1]))\n",
    "\n",
    "    \n",
    "    # Output layer\n",
    "    if (regression) :      # 0 represents Regression, 1 represents Classification\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])\n",
    "    else :\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure\n",
    "1. load x (path) and y (label) data \n",
    "2. define training batches\n",
    "3. implement the training model defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_Procedure :\n",
    "    \n",
    "    def __init__(self, model, image_size, train_input_path, train_output, test_input_path, test_output, batch_size, epochs, regression=True) :\n",
    "\n",
    "        self.model = model\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.train_input_path = train_input_path\n",
    "        self.train_output = train_output\n",
    "\n",
    "        self.test_input_path = test_input_path\n",
    "        self.test_output = test_output\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.regression = regression\n",
    "\n",
    "    # def load_image(path, num_arr) :\n",
    "    #     images = []\n",
    "    #     for num in num_arr :\n",
    "    #         images.append(np.asarray(Image(path + \"/\" + num + \".jpg\")))\n",
    "\n",
    "    #     return np.asarray(images)\n",
    "\n",
    "\n",
    "    # have yet known how to load data for regression \n",
    "    #   since the label values must be type integers\n",
    "    def load_train_data(self, sub_set=\"training\", val_split=0.2) :\n",
    "        train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            self.train_input_path,\n",
    "            labels=list(self.train_output.astype(int)),\n",
    "            color_mode='rgb',\n",
    "            # Use 20% data as testing data.\n",
    "            validation_split=val_split,\n",
    "            subset=sub_set,\n",
    "            # Set seed to ensure the same split when loading testing data.\n",
    "            seed=123,\n",
    "            image_size=(self.image_size, self.image_size),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return train_data\n",
    "\n",
    "\n",
    "\n",
    "    def load_test_data(self) :\n",
    "        test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            self.test_input_path,\n",
    "            labels=list(self.test_output.astype(int)),\n",
    "            color_mode='rgb',\n",
    "            # Set seed to ensure the same split when loading testing data.\n",
    "            seed=123,\n",
    "            image_size=(self.image_size, self.image_size),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return test_data\n",
    "\n",
    "\n",
    "    def process_classification(self) :\n",
    "\n",
    "        train_data = self.load_train_data()\n",
    "        val_data = self.load_train_data(sub_set=\"validation\")\n",
    "        test_data = self.load_test_data()\n",
    "\n",
    "        # Training process\n",
    "        startTime = timeit.default_timer()\n",
    "        hist = self.model.fit(train_data,\n",
    "                        validation_data=val_data,\n",
    "                        epochs=self.epochs,\n",
    "                        batch_size=self.batch_size, verbose=1)\n",
    "        print('Training time:', timeit.default_timer() - startTime)\n",
    "\n",
    "        # Testing\n",
    "        startTime = timeit.default_timer()\n",
    "        accuracy = self.model.evaluate(test_data)\n",
    "        print('Testing time:', timeit.default_timer() - startTime)\n",
    "\n",
    "        return hist.history, accuracy, self.model.count_params()\n",
    "\n",
    "\n",
    "    def process(self) :\n",
    "        if (not self.regression) : \n",
    "            return self.process_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() :\n",
    "    model = alexnet_var_model()\n",
    "    train_input_path = \"C:/Phanh/BuAnhNet/EAAI23/data/train_set/New Folder\"\n",
    "    train_output = np.load(\"C:/Phanh/BuAnhNet/EAAI23/data/trainval_class.npy\")\n",
    "\n",
    "    test_input_path = \"C:/Phanh/BuAnhNet/EAAI23/data/val_set/New Folder\"\n",
    "    test_output = np.load(\"C:/Phanh/BuAnhNet/EAAI23/data/test_class_sorted.npy\")\n",
    "    batch_size = 400 \n",
    "    epochs = 24\n",
    "\n",
    "    training_procedure = Training_Procedure(\n",
    "        model, \n",
    "        image_size=128, \n",
    "        train_input_path=train_input_path, \n",
    "        train_output=train_output, \n",
    "        test_input_path=test_input_path,\n",
    "        test_output=test_output,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        regression=False    \n",
    "    )\n",
    "\n",
    "    model.save('AlexNet_Variant_20220227')\n",
    "    hist = training_procedure.process()\n",
    "\n",
    "    train_loss = hist.history['loss']\n",
    "    train_acc = hist.history['accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "\n",
    "    viz_res(train_loss, train_acc, val_loss, val_acc, 'alexnet_var_20220227')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# len(os.listdir(\"C:/Phanh/train_set/train_set/images\"))\n",
    "# os.getcwd()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization\n",
    "Visualize the training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_res(trainLoss, trainAcc, valLoss, valAcc, savename = None):\n",
    "    plt.figure(figsize=(15,7))\n",
    "\n",
    "    plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "    plt.plot(trainLoss, color='#17bccf', label='Train')\n",
    "    plt.plot(valLoss, color='#ff7f44', label='Val')\n",
    "    plt.title(\"Loss Func\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2) # index 2\n",
    "    plt.plot(trainAcc, color='#17bccf', label='Train')\n",
    "    plt.plot(valAcc, color='#ff7f44', label='Val')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savename != None: plt.savefig('./figure/' + savename + '.jpg', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "WARNING:tensorflow:From C:\\Users\\Anh Do\\AppData\\Local\\Temp\\ipykernel_16264\\3654935557.py:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e28bfe5e849496f1d027a6bcc8730a360d479bceb67f0622f77bd617aebb68f4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
